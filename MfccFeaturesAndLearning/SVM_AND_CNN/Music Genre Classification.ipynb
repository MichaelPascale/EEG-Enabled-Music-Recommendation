{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\arjun\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.28.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: gast==0.2.2 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (46.1.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.1.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\arjun\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 20.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataset\\nWe use GTZAN genre collection dataset for classification. \\n\\nThe dataset consists of 10 genres i.e\\n\\nBlues\\nClassical\\nCountry\\nDisco\\nHiphop\\nJazz\\nMetal\\nPop\\nReggae\\nRock\\nEach genre contains 100 songs. Total dataset: 1000 songs\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting music and features\n",
    "'''\n",
    "Dataset\n",
    "We use GTZAN genre collection dataset for classification. \n",
    "\n",
    "The dataset consists of 10 genres i.e\n",
    "\n",
    "Blues\n",
    "Classical\n",
    "Country\n",
    "Disco\n",
    "Hiphop\n",
    "Jazz\n",
    "Metal\n",
    "Pop\n",
    "Reggae\n",
    "Rock\n",
    "Each genre contains 100 songs. Total dataset: 1000 songs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Spectrogram for every Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncmap = plt.get_cmap(\\'inferno\\')\\n\\nplt.figure(figsize=(10,10))\\ngenres = \\'blues classical country disco hiphop jazz metal pop reggae rock\\'.split()\\nfor g in genres:\\n    pathlib.Path(f\\'img_data/{g}\\').mkdir(parents=True, exist_ok=True)     \\n    for filename in os.listdir(f\\'./MIR/genres/{g}\\'):\\n        songname = f\\'./MIR/genres/{g}/{filename}\\'\\n        y, sr = librosa.load(songname, mono=True, duration=5)\\n        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides=\\'default\\', mode=\\'default\\', scale=\\'dB\\');\\n        plt.axis(\\'off\\');\\n        plt.savefig(f\\'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png\\')\\n        plt.clf()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cmap = plt.get_cmap('inferno')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'./MIR/genres/{g}'):\n",
    "        songname = f'./MIR/genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=5)\n",
    "        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
    "        plt.axis('off');\n",
    "        plt.savefig(f'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExtracting features from Spectrogram\\nWe will extract\\n\\nMel-frequency cepstral coefficients (MFCC)(20 in number)\\nSpectral Centroid,\\nZero Crossing Rate\\nChroma Frequencies\\nSpectral Roll-off.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Extracting features from Spectrogram\n",
    "We will extract\n",
    "\n",
    "Mel-frequency cepstral coefficients (MFCC)(20 in number)\n",
    "Spectral Centroid,\n",
    "Zero Crossing Rate\n",
    "Chroma Frequencies\n",
    "Spectral Roll-off.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfile = open('data.csv', 'w', newline='')\\nwith file:\\n    writer = csv.writer(file)\\n    writer.writerow(header)\\ngenres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\\nfor g in genres:\\n    for filename in os.listdir(f'./MIR/genres/{g}'):\\n        songname = f'./MIR/genres/{g}/{filename}'\\n        y, sr = librosa.load(songname, mono=True, duration=30)\\n        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\\n        rmse = librosa.feature.rmse(y=y)\\n        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\\n        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\\n        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\\n        zcr = librosa.feature.zero_crossing_rate(y)\\n        mfcc = librosa.feature.mfcc(y=y, sr=sr)\\n        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \\n        for e in mfcc:\\n            to_append += f' {np.mean(e)}'\\n        to_append += f' {g}'\\n        file = open('data.csv', 'a', newline='')\\n        with file:\\n            writer = csv.writer(file)\\n            writer.writerow(to_append.split())\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('data.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    for filename in os.listdir(f'./MIR/genres/{g}'):\n",
    "        songname = f'./MIR/genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        rmse = librosa.feature.rmse(y=y)\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g}'\n",
    "        file = open('data.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>rmse</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>mfcc3</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00060.au</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.196222</td>\n",
       "      <td>1946.565652</td>\n",
       "      <td>1979.909934</td>\n",
       "      <td>3955.867746</td>\n",
       "      <td>0.097454</td>\n",
       "      <td>-67.770980</td>\n",
       "      <td>111.704184</td>\n",
       "      <td>-34.646105</td>\n",
       "      <td>...</td>\n",
       "      <td>12.295832</td>\n",
       "      <td>-12.477988</td>\n",
       "      <td>1.681278</td>\n",
       "      <td>-5.142068</td>\n",
       "      <td>4.644002</td>\n",
       "      <td>-6.919217</td>\n",
       "      <td>1.040718</td>\n",
       "      <td>-4.736871</td>\n",
       "      <td>-0.660037</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00082.au</td>\n",
       "      <td>0.338896</td>\n",
       "      <td>0.251350</td>\n",
       "      <td>2141.461656</td>\n",
       "      <td>2168.015560</td>\n",
       "      <td>4627.997015</td>\n",
       "      <td>0.105151</td>\n",
       "      <td>-29.362093</td>\n",
       "      <td>108.667950</td>\n",
       "      <td>-25.573165</td>\n",
       "      <td>...</td>\n",
       "      <td>5.456504</td>\n",
       "      <td>-7.687713</td>\n",
       "      <td>7.410600</td>\n",
       "      <td>-11.319177</td>\n",
       "      <td>7.229288</td>\n",
       "      <td>-9.466552</td>\n",
       "      <td>1.930059</td>\n",
       "      <td>-6.328476</td>\n",
       "      <td>-1.304812</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00030.au</td>\n",
       "      <td>0.263016</td>\n",
       "      <td>0.170081</td>\n",
       "      <td>1379.081742</td>\n",
       "      <td>2004.000850</td>\n",
       "      <td>3015.831764</td>\n",
       "      <td>0.039376</td>\n",
       "      <td>-206.987590</td>\n",
       "      <td>117.781468</td>\n",
       "      <td>23.256245</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.015467</td>\n",
       "      <td>-17.616342</td>\n",
       "      <td>-8.138554</td>\n",
       "      <td>-8.646157</td>\n",
       "      <td>-15.538988</td>\n",
       "      <td>-15.331506</td>\n",
       "      <td>-9.664872</td>\n",
       "      <td>-10.103310</td>\n",
       "      <td>-17.835100</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00007.au</td>\n",
       "      <td>0.307921</td>\n",
       "      <td>0.131785</td>\n",
       "      <td>1451.754147</td>\n",
       "      <td>1577.369917</td>\n",
       "      <td>2955.348796</td>\n",
       "      <td>0.061435</td>\n",
       "      <td>-179.395447</td>\n",
       "      <td>136.459244</td>\n",
       "      <td>-26.656359</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.954827</td>\n",
       "      <td>-3.544535</td>\n",
       "      <td>-8.051242</td>\n",
       "      <td>-8.959537</td>\n",
       "      <td>-8.424337</td>\n",
       "      <td>-10.558885</td>\n",
       "      <td>-10.788159</td>\n",
       "      <td>-4.693749</td>\n",
       "      <td>-8.638613</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00064.au</td>\n",
       "      <td>0.332480</td>\n",
       "      <td>0.117413</td>\n",
       "      <td>2553.232415</td>\n",
       "      <td>2280.128669</td>\n",
       "      <td>5148.102203</td>\n",
       "      <td>0.146852</td>\n",
       "      <td>-85.150250</td>\n",
       "      <td>88.806722</td>\n",
       "      <td>-16.322611</td>\n",
       "      <td>...</td>\n",
       "      <td>8.478453</td>\n",
       "      <td>-19.590226</td>\n",
       "      <td>6.413210</td>\n",
       "      <td>-13.779667</td>\n",
       "      <td>6.112037</td>\n",
       "      <td>-13.154644</td>\n",
       "      <td>3.933456</td>\n",
       "      <td>-7.615454</td>\n",
       "      <td>3.752626</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  chroma_stft      rmse  spectral_centroid  \\\n",
       "0  blues.00060.au     0.430894  0.196222        1946.565652   \n",
       "1  blues.00082.au     0.338896  0.251350        2141.461656   \n",
       "2  blues.00030.au     0.263016  0.170081        1379.081742   \n",
       "3  blues.00007.au     0.307921  0.131785        1451.754147   \n",
       "4  blues.00064.au     0.332480  0.117413        2553.232415   \n",
       "\n",
       "   spectral_bandwidth      rolloff  zero_crossing_rate       mfcc1  \\\n",
       "0         1979.909934  3955.867746            0.097454  -67.770980   \n",
       "1         2168.015560  4627.997015            0.105151  -29.362093   \n",
       "2         2004.000850  3015.831764            0.039376 -206.987590   \n",
       "3         1577.369917  2955.348796            0.061435 -179.395447   \n",
       "4         2280.128669  5148.102203            0.146852  -85.150250   \n",
       "\n",
       "        mfcc2      mfcc3  ...     mfcc12     mfcc13    mfcc14     mfcc15  \\\n",
       "0  111.704184 -34.646105  ...  12.295832 -12.477988  1.681278  -5.142068   \n",
       "1  108.667950 -25.573165  ...   5.456504  -7.687713  7.410600 -11.319177   \n",
       "2  117.781468  23.256245  ...  -8.015467 -17.616342 -8.138554  -8.646157   \n",
       "3  136.459244 -26.656359  ...  -6.954827  -3.544535 -8.051242  -8.959537   \n",
       "4   88.806722 -16.322611  ...   8.478453 -19.590226  6.413210 -13.779667   \n",
       "\n",
       "      mfcc16     mfcc17     mfcc18     mfcc19     mfcc20  label  \n",
       "0   4.644002  -6.919217   1.040718  -4.736871  -0.660037  blues  \n",
       "1   7.229288  -9.466552   1.930059  -6.328476  -1.304812  blues  \n",
       "2 -15.538988 -15.331506  -9.664872 -10.103310 -17.835100  blues  \n",
       "3  -8.424337 -10.558885 -10.788159  -4.693749  -8.638613  blues  \n",
       "4   6.112037 -13.154644   3.933456  -7.615454   3.752626  blues  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unneccesary columns\n",
    "data = data.drop(['filename'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data into training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.47381459,  0.55637099,  1.03155087,  0.7354193 ,  0.74898954,\n",
       "        1.00725641,  0.56680776, -1.12527311, -0.18683187, -1.21614088,\n",
       "        0.1334671 , -1.20240975,  0.96203517, -1.22801527,  0.80756641,\n",
       "       -0.98014561,  0.01410241, -1.30172897,  0.20846547, -1.28134964,\n",
       "       -0.04602419, -0.97537067,  0.91973706,  0.56842323,  1.28358311,\n",
       "        0.34215076])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification with Keras\n",
    "# Building our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM for text data\n",
      "[5 0 6 2 6 8 6 3 7 7 0 8 2 4 2 3 7 6 5 8 3 5 9 6 2 2 1 5 4 1 5 4 3 5 4 3 0\n",
      " 6 5 2 7 6 5 3 0 0 7 3 6 5 6 1 2 6 3 2 4 5 9 2 1 2 9 3 4 3 1 1 4 9 7 1 3 0\n",
      " 5 3 5 9 2 4 8 1 1 0 5 3 7 5 2 7 5 3 7 0 1 0 9 2 0 2 0 7 2 1 3 8 3 7 4 6 8\n",
      " 9 6 9 4 2 5 1 8 2 9 3 1 1 0 1 3 4 0 2 2 2 2 1 8 2 8 8 5 9 7 7 2 1 2 1 6 0\n",
      " 0 5 3 4 2 2 3 1 6 5 5 2 3 3 9 4 7 3 0 5 0 0 5 6 0 0 9 3 6 8 2 4 0 3 8 2 2\n",
      " 1 4 5 0 1 4 1 5 7 1 1 3 0 4 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.615"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "svb = svm.SVC(kernel='linear')\n",
    "svb.fit(X=X_train, y=y_train)\n",
    "results3 = svb.predict(X_test)\n",
    "\n",
    "print(\"SVM for text data\")\n",
    "print(results3)\n",
    "\n",
    "metrics.accuracy_score(y_test, results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "800/800 [==============================] - 0s 271us/step - loss: 2.1515 - accuracy: 0.2600\n",
      "Epoch 2/20\n",
      "800/800 [==============================] - 0s 29us/step - loss: 1.8586 - accuracy: 0.3713\n",
      "Epoch 3/20\n",
      "800/800 [==============================] - 0s 32us/step - loss: 1.6442 - accuracy: 0.4137\n",
      "Epoch 4/20\n",
      "800/800 [==============================] - 0s 27us/step - loss: 1.4637 - accuracy: 0.4825\n",
      "Epoch 5/20\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.3108 - accuracy: 0.5238\n",
      "Epoch 6/20\n",
      "800/800 [==============================] - 0s 31us/step - loss: 1.1980 - accuracy: 0.5688\n",
      "Epoch 7/20\n",
      "800/800 [==============================] - 0s 26us/step - loss: 1.1137 - accuracy: 0.6125\n",
      "Epoch 8/20\n",
      "800/800 [==============================] - 0s 34us/step - loss: 1.0470 - accuracy: 0.6363\n",
      "Epoch 9/20\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.9864 - accuracy: 0.6725\n",
      "Epoch 10/20\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.9333 - accuracy: 0.6925\n",
      "Epoch 11/20\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.8846 - accuracy: 0.7063\n",
      "Epoch 12/20\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.8349 - accuracy: 0.7200\n",
      "Epoch 13/20\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.7941 - accuracy: 0.7337\n",
      "Epoch 14/20\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.7544 - accuracy: 0.7475\n",
      "Epoch 15/20\n",
      "800/800 [==============================] - 0s 21us/step - loss: 0.7251 - accuracy: 0.7638\n",
      "Epoch 16/20\n",
      "800/800 [==============================] - 0s 22us/step - loss: 0.6927 - accuracy: 0.7713\n",
      "Epoch 17/20\n",
      "800/800 [==============================] - 0s 21us/step - loss: 0.6643 - accuracy: 0.7912\n",
      "Epoch 18/20\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.6303 - accuracy: 0.8025\n",
      "Epoch 19/20\n",
      "800/800 [==============================] - 0s 24us/step - loss: 0.6030 - accuracy: 0.8025\n",
      "Epoch 20/20\n",
      "800/800 [==============================] - 0s 25us/step - loss: 0.5764 - accuracy: 0.8188\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 125us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc:  0.6650000214576721\n"
     ]
    }
   ],
   "source": [
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tes accuracy is less than training data accuracy. This hints at Overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating our approach\n",
    "# Let's set apart 200 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:200]\n",
    "partial_x_train = X_train[200:]\n",
    "\n",
    "y_val = y_train[:200]\n",
    "partial_y_train = y_train[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/30\n",
      "600/600 [==============================] - 0s 283us/step - loss: 2.2690 - accuracy: 0.1650 - val_loss: 2.1379 - val_accuracy: 0.3500\n",
      "Epoch 2/30\n",
      "600/600 [==============================] - 0s 47us/step - loss: 2.1133 - accuracy: 0.3650 - val_loss: 2.0114 - val_accuracy: 0.3300\n",
      "Epoch 3/30\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.9751 - accuracy: 0.3717 - val_loss: 1.8892 - val_accuracy: 0.3450\n",
      "Epoch 4/30\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.8323 - accuracy: 0.3767 - val_loss: 1.7906 - val_accuracy: 0.3500\n",
      "Epoch 5/30\n",
      "600/600 [==============================] - 0s 35us/step - loss: 1.6991 - accuracy: 0.4033 - val_loss: 1.6999 - val_accuracy: 0.4100\n",
      "Epoch 6/30\n",
      "600/600 [==============================] - 0s 33us/step - loss: 1.5703 - accuracy: 0.4483 - val_loss: 1.6167 - val_accuracy: 0.4500\n",
      "Epoch 7/30\n",
      "600/600 [==============================] - 0s 35us/step - loss: 1.4528 - accuracy: 0.5100 - val_loss: 1.5253 - val_accuracy: 0.4700\n",
      "Epoch 8/30\n",
      "600/600 [==============================] - 0s 43us/step - loss: 1.3459 - accuracy: 0.5517 - val_loss: 1.4619 - val_accuracy: 0.4350\n",
      "Epoch 9/30\n",
      "600/600 [==============================] - 0s 38us/step - loss: 1.2606 - accuracy: 0.5700 - val_loss: 1.4262 - val_accuracy: 0.4300\n",
      "Epoch 10/30\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.1759 - accuracy: 0.6117 - val_loss: 1.3923 - val_accuracy: 0.4950\n",
      "Epoch 11/30\n",
      "600/600 [==============================] - 0s 40us/step - loss: 1.0967 - accuracy: 0.6633 - val_loss: 1.3832 - val_accuracy: 0.5250\n",
      "Epoch 12/30\n",
      "600/600 [==============================] - 0s 35us/step - loss: 1.0546 - accuracy: 0.6600 - val_loss: 1.3429 - val_accuracy: 0.5550\n",
      "Epoch 13/30\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.9882 - accuracy: 0.6833 - val_loss: 1.3241 - val_accuracy: 0.5500\n",
      "Epoch 14/30\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.9402 - accuracy: 0.6900 - val_loss: 1.3442 - val_accuracy: 0.5350\n",
      "Epoch 15/30\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.9095 - accuracy: 0.7167 - val_loss: 1.3686 - val_accuracy: 0.5300\n",
      "Epoch 16/30\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.8881 - accuracy: 0.7250 - val_loss: 1.3284 - val_accuracy: 0.5650\n",
      "Epoch 17/30\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.8195 - accuracy: 0.7400 - val_loss: 1.2954 - val_accuracy: 0.5850\n",
      "Epoch 18/30\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.7964 - accuracy: 0.7400 - val_loss: 1.2616 - val_accuracy: 0.5900\n",
      "Epoch 19/30\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.7750 - accuracy: 0.7517 - val_loss: 1.2680 - val_accuracy: 0.5850\n",
      "Epoch 20/30\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.7265 - accuracy: 0.7683 - val_loss: 1.3308 - val_accuracy: 0.5500\n",
      "Epoch 21/30\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.7238 - accuracy: 0.7733 - val_loss: 1.2865 - val_accuracy: 0.5750\n",
      "Epoch 22/30\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.6659 - accuracy: 0.7800 - val_loss: 1.2687 - val_accuracy: 0.5950\n",
      "Epoch 23/30\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.6598 - accuracy: 0.8000 - val_loss: 1.2582 - val_accuracy: 0.6100\n",
      "Epoch 24/30\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.6383 - accuracy: 0.8000 - val_loss: 1.2494 - val_accuracy: 0.5950\n",
      "Epoch 25/30\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.6090 - accuracy: 0.8283 - val_loss: 1.2438 - val_accuracy: 0.5950\n",
      "Epoch 26/30\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.5799 - accuracy: 0.8183 - val_loss: 1.2490 - val_accuracy: 0.6000\n",
      "Epoch 27/30\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.5776 - accuracy: 0.8133 - val_loss: 1.2616 - val_accuracy: 0.6300\n",
      "Epoch 28/30\n",
      "600/600 [==============================] - 0s 35us/step - loss: 0.5663 - accuracy: 0.8267 - val_loss: 1.2774 - val_accuracy: 0.6000\n",
      "Epoch 29/30\n",
      "600/600 [==============================] - 0s 40us/step - loss: 0.5325 - accuracy: 0.8350 - val_loss: 1.3193 - val_accuracy: 0.5700\n",
      "Epoch 30/30\n",
      "600/600 [==============================] - 0s 37us/step - loss: 0.5198 - accuracy: 0.8333 - val_loss: 1.3084 - val_accuracy: 0.5700\n",
      "200/200 [==============================] - 0s 65us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=30,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0994455432891845, 0.6499999761581421]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
